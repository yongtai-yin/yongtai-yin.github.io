---
layout: post
title: "IERG 5330 Reinforcement Learning"
date: 2026-01-27
tags: [reinforcement learning]
katex: true
---

Notes for [Mathematical Foundations of Reinforcement Learning](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning).

## Basic Concepts
### State and action

- **State**: the agentâ€™s status with respect to the environment.
- The set of all the states is called the state space, denoted as $\mathcal{S}$.
- Different states can have different action spaces.

### State transition

When taking an action, the agent may move from one state to another. Such a process is called *state transition*. For example, if the agent is in state $s_1$ and selects action $a_2$, then the agent moves to state $s_2$. Such a process can be expressed as

$$
s_1 \xrightarrow{a_2} s_2
$$

In general, state transitions can be *stochastic* and must be described by conditional probability distributions.

### Policy

- A policy tells the agent which actions to take at every state.
- Following a policy, the agent can generate a trajectory starting from an initial state
- Mathematically, policies can be described by conditional probabilities, denoted as $\pi(a \mid s)$, which is a conditional probability distribution function defined for every state.

### Reward

- After executing an action at a state, the agent obtains a reward, denoted as $r$, as feedback from the environment.
- The reward is a function of the state $s$ and action $a$. Hence, it is also denoted as $r(s, a)$.
- A reward can be interpreted as a human-machine interface, with which we can guide the agent to behave as we expect.
- Designing appropriate rewards is an important step in reinforcement learning. This step is, however, nontrivial for complex tasks since it may require the user to understand the given problem well.
- To determine a good policy, we must consider the *total reward* obtained in the long run. An action with the greatest immediate reward may not lead to the greatest total reward.
- A general approach is to use conditional probabilities $p(r \mid s, a)$ to describe reward processes.

### Trajectories, returns, and episodes

A trajectory is a state-action-reward chain:

$$
s_1 \xrightarrow[r=0]{a_2} s_2 \xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8 \xrightarrow[r=1]{a_2} s_9
$$

The return of this trajectory is defined as the sum of all the rewards collected along the trajectory:

$$
\text{return} = 0+0+0+1=1
$$

Returns are also called *total rewards* or *cumulative rewards*.

Returns can be used to evaluate policies.

A return consists of an *immediate reward* and *future rewards*. Here, the immediate reward is the reward obtained after taking an action at the initial state; the future rewards refer to the rewards obtained after leaving the initial state.

Return can also be defined for infinitely long trajectories.

To avoid divergence, we must introduce the *discounted return* concept for infinitely long trajectories:

$$
\text{discounted return} = 0 + \gamma 0 + \gamma^2 0 + \gamma^3 1 + \gamma^4 1 + \gamma^5 1 + \dots
$$

where $\gamma \in (0, 1)$ is called the *discount rate*.

The introduction of the discount rate is useful for the following reasons.
- First, it removes the stop criterion and allows for infinitely long trajectories.
- Second, the discount rate can be used to adjust the emphasis placed on near- or far-future rewards.

When interacting with the environment by following a policy, the agent may stop at some *terminal states*. The resulting trajectory is called an *episode* (or a trial).

An episode is usually assumed to be a finite trajectory. Tasks with episodes are called *episodic tasks*. However, some tasks may have no terminal states, meaning that the process of interacting with the environment will never end. Such tasks are called *continuing tasks*.

### Markov decision processes

An Markov decision process (MDP) is a general framework for describing stochastic dynamical systems. The key ingredients of an MDP are listed below.

- Sets:
	- State space: the set of all states, denoted as $\mathcal{S}$.
	- Action space: a set of actions, denoted as $\mathcal{A}(s)$, associated with each state $s \in \mathcal{S}$.
	- Reward set: a set of rewards, denoted as $\mathcal{R}(s, a)$, associated with each state-action pair $(s, a)$.
- Models:
	- State transition probability: In state $s$, when taking action $a$, the probability of transitioning to state $s'$ is $p(s' \mid s, a)$. It holds that $\sum_{s' \in \mathcal{S}} p(s' \mid s, a) = 1$ for any $(s, a)$.
	- Reward probability: In state $s$, when taking action $a$, the probability of obtaining reward $r$ is $p(r \mid s, a)$. It holds that $\sum_{r \in \mathcal{R}(s, a)} p(r \mid s, a) = 1$ for any $(s, a)$.
- Policy: In state $s$, the probability of choosing action $a$ is $\pi(a \mid s)$. It holds that $\sum_{a \in \mathcal{A}(s)} \pi(a \mid s) = 1$ for any $s \in \mathcal{S}$.
- Markov property: The *Markov property* refers to the memoryless property of a stochastic process. Mathematically, it means that

$$
\begin{aligned}
	p(s_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \dots, s_0, a_0) &= p(s_{t+1} \mid s_t, a_t), \\
	p(r_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \dots, s_0, a_0) &= p(r_{t+1} \mid s_t, a_t),
\end{aligned}
$$

where $t$ represents the current time step and $t + 1$ represents the next time step. This indicates that the next state or reward depends merely on the current state and action and is independent of the previous ones.

## State Values and Bellman Equation