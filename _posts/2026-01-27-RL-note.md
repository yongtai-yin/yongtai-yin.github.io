---
layout: post
title: "IERG 5330 Reinforcement Learning"
date: 2026-01-27
tags: [reinforcement learning]
katex: true
---

Notes for [Mathematical Foundations of Reinforcement Learning](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning).

## Basic Concepts
### State and action

- **State**: the agentâ€™s status with respect to the environment.
- The set of all the states is called the state space, denoted as $\mathcal{S}$.
- Different states can have different action spaces.

### State transition

When taking an action, the agent may move from one state to another. Such a process is called *state transition*. For example, if the agent is in state $s_1$ and selects action $a_2$, then the agent moves to state $s_2$. Such a process can be expressed as
$$
s_1 \xrightarrow{a_2} s_2
$$

In general, state transitions can be *stochastic* and must be described by conditional probability distributions.

### Policy

- A policy tells the agent which actions to take at every state.
- Following a policy, the agent can generate a trajectory starting from an initial state
- Mathematically, policies can be described by conditional probabilities, denoted as $\pi(a | s)$, which is a conditional probability distribution function defined for every state.

### Reward

- After executing an action at a state, the agent obtains a reward, denoted as $r$, as feedback from the environment.
- The reward is a function of the state $s$ and action $a$. Hence, it is also denoted as $r(s, a)$.
- A reward can be interpreted as a human-machine interface, with which we can guide the agent to behave as we expect.
- Designing appropriate rewards is an important step in reinforcement learning. This step is, however, nontrivial for complex tasks since it may require the user to understand the given problem well.
- To determine a good policy, we must consider the *total reward* obtained in the long run. An action with the greatest immediate reward may not lead to the greatest total reward.
- A general approach is to use conditional probabilities $p(r | s, a)$ to describe reward processes.

### Trajectories, returns, and episodes

A trajectory is a state-action-reward chain:
$$
s_1 \xrightarrow[r=0]{a_2} s_2 \xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8 \xrightarrow[r=1]{a_2} s_9
$$

The return of this trajectory is defined as the sum of all the rewards collected along the trajectory:
$$
\text{return} = 0+0+0+1=1
$$
Returns are also called *total rewards* or *cumulative rewards*.

## State Values and Bellman Equation